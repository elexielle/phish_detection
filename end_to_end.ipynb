{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end to end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path = r\"C:\\Users\\exusille\\Documents\\school\\thesis\\vsc thesis workspace\\NEW_WORKSPACE\\dataset_v5\\links_v5.2.csv\"\n",
    "df = pd.read_csv(path)\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from urlparse import urlparse\n",
    "from urllib import parse, request\n",
    "#from urllib.parse import urlparse\n",
    "from typing import Optional, Dict\n",
    "\n",
    "def parse_url(url: str) -> Optional[Dict[str, str]]:\n",
    "    try:\n",
    "        no_scheme = not url.startswith('https://') and not url.startswith('http://')\n",
    "        if no_scheme:\n",
    "            parsed_url = urlparse(f\"http://{url}\")\n",
    "            url_dict = {\n",
    "                #\"scheme\": None, # not established a value for this\n",
    "                \"netloc\": parsed_url.netloc,\n",
    "                \"domain\": parsed_url.netloc.split(':')[0], # extract domain from netloc\n",
    "                \"path\": parsed_url.path,\n",
    "                \"params\": parsed_url.params,\n",
    "                \"query\": parsed_url.query,\n",
    "                \"fragment\": parsed_url.fragment,\n",
    "            }\n",
    "        else:\n",
    "            parsed_url = urlparse(url)\n",
    "            url_dict = {\n",
    "                \"scheme\": parsed_url.scheme,\n",
    "                \"netloc\": parsed_url.netloc,\n",
    "                \"domain\": parsed_url.netloc.split(':')[0], # extract domain from netloc\n",
    "                \"path\": parsed_url.path,\n",
    "                \"params\": parsed_url.params,\n",
    "                \"query\": parsed_url.query,\n",
    "                \"fragment\": parsed_url.fragment,\n",
    "            }\n",
    "\n",
    "        # Split path into directory and file\n",
    "        directory, file = parsed_url.path.rsplit('/', 1)\n",
    "        url_dict['directory'] = directory\n",
    "        url_dict['file'] = file\n",
    "\n",
    "        return url_dict\n",
    "\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['parsed_url'] = df.url.apply(parse_url) #parse urls in url_df\n",
    "df = pd.concat([\n",
    "    df.drop(['parsed_url'], axis=1),\n",
    "    df['parsed_url'].apply(pd.Series)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_length(row):\n",
    "    return pd.Series({\n",
    "        'url_length': len(row['url']),\n",
    "        'domain_length': len(row['domain'])\n",
    "    })\n",
    "length_df = df.apply(get_length, axis=1)\n",
    "df = df.merge(length_df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tldextract\n",
    "#tld\n",
    "df[\"tld\"] = df.netloc.apply(lambda nl: tldextract.extract(nl).suffix)\n",
    "df['tld'] = df['tld'].replace('','None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = '.-'\n",
    "for char in symbols:\n",
    "    df['qnty_' + char + '_url'] = df['url'].apply(lambda x: x.count(char))\n",
    "    df['qnty_' + char + '_domain'] = df['domain'].apply(lambda x: x.count(char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##server domain\n",
    "def extract_server_client_domain(domains):\n",
    "    results = []\n",
    "    for domain in domains:\n",
    "        parts = domain.split('.')\n",
    "        if len(parts) == 3 and parts[0] == 'server':\n",
    "            results.append(1)\n",
    "        elif len(parts) == 2 and parts[0] != 'www':\n",
    "            results.append(0)\n",
    "        else:\n",
    "            results.append(-1)  # invalid domain format\n",
    "    return results\n",
    "df['is_server_domain'] = extract_server_client_domain(df['domain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_subdomains(netloc: str) -> int:\n",
    "    subdomain = tldextract.extract(netloc).subdomain \n",
    "    if subdomain == \"\":\n",
    "        return 0\n",
    "    return subdomain.count('.') + 1\n",
    "df['num_subdomains'] = df['netloc'].apply(lambda net: get_num_subdomains(net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'[A-Za-z]+')\n",
    "def tokenize_domain(netloc: str) -> str:\n",
    "    split_domain = tldextract.extract(netloc)\n",
    "    no_tld = str(split_domain.subdomain +'.'+ split_domain.domain)\n",
    "    return \" \".join(map(str,tokenizer.tokenize(no_tld)))\n",
    "         \n",
    "df['domain_tokens'] = df['netloc'].apply(lambda net: tokenize_domain(net))\n",
    "df['path_tokens'] = df['path'].apply(lambda path: \" \".join(map(str,tokenizer.tokenize(path))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tok= RegexpTokenizer(r'[A-Za-z0-9]+')\n",
    "df['tokenized_url'] = df['url'].map(lambda x: tok.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unecessary columns\n",
    "cols_to_drop = ['netloc', 'domain', 'path', 'params', 'query', 'scheme',\n",
    "       'fragment', 'directory', 'file', 'path_tokens']\n",
    "df.drop(cols_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label_id\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "y = df['label']\n",
    "le = LabelEncoder()\n",
    "le.fit(y)\n",
    "y = le.transform(y)\n",
    "df['label_id'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['label', 'label_id', 'url'])\n",
    "y = df['label_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class Converter(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_frame):\n",
    "        return data_frame.values.ravel()\n",
    "    \n",
    "    \n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "numeric_features = X.select_dtypes(include=['int64']).columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', MinMaxScaler())])\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "categorical_features = ['tld']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "vectorizer_features = ['domain_tokens']#, 'tokenized_url']\n",
    "vectorizer_transformer = Pipeline(steps=[\n",
    "    ('con', Converter()),\n",
    "    ('tf', TfidfVectorizer())])\n",
    "\n",
    "urltoken_features = ['tokenized_url']#X.select_dtypes(include=['float64']).columns\n",
    "urltoken_transformer = Pipeline(steps=[\n",
    "    ('con', Converter()),\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(1, 1), max_features=1000)),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        ('domvec', vectorizer_transformer, ['domain_tokens']),\n",
    "        ('urlvec', urltoken_transformer, urltoken_features)\n",
    "    ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_hyp = {\n",
    "    'C': 25.406936492978463,\n",
    "    'coef0': 6,\n",
    "    'degree': 2,\n",
    "    'gamma': 1.3190980166240491,\n",
    "    'kernel': 'rbf',\n",
    "    'probability': True,\n",
    "    'shrinking': True\n",
    "}\n",
    "model = Pipeline(steps=[('preprocessor', preprocessor), \n",
    "                              ('classifier', SVC(**svc_hyp, random_state=42))\n",
    "                               ])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, f1_score, precision_score, recall_score\n",
    "import pandas as pd\n",
    "\n",
    "CV = 10\n",
    "entries = []\n",
    "\n",
    "# Perform cross-validation training\n",
    "cv_scores = cross_val_score(model, X, y, cv=CV)\n",
    "\n",
    "# Fit the model on the entire dataset\n",
    "model.fit(X, y)\n",
    "\n",
    "# Make predictions and calculate scores\n",
    "y_pred = model.predict(X)\n",
    "y_scores = model.decision_function(X)\n",
    "probs = model.predict_proba(X)[:, 1]\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "conf_matrix = confusion_matrix(y, y_pred)\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "roc_auc = roc_auc_score(y, y_scores)\n",
    "f1 = f1_score(y, y_pred)\n",
    "precision = precision_score(y, y_pred)\n",
    "recall = recall_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Store the results in the entries list\n",
    "# entries.append((accuracy, roc_auc, f1, precision, recall))\n",
    "\n",
    "# # Print confusion matrix\n",
    "# print(\"Confusion matrix for SVM\")\n",
    "# print(conf_matrix)\n",
    "\n",
    "# # Create a DataFrame with the results\n",
    "# bayes_df = pd.DataFrame(entries, columns=['accuracy', 'roc_auc', 'f1', 'precision', 'recall'])\n",
    "# bayes_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9997414461777127\n",
      "Recall: 0.9993969156543465\n",
      "F1-score: 0.999569151227919\n",
      "Accuracy: 0.9995619414753811\n",
      "ROC AUC Score: 0.9999207361039587\n",
      "False Negative Rate: 0.0006030843456534849\n",
      "False Positive Rate: 0.0002673558506371981\n",
      "Confusion Matrix:\n",
      "[[11218     3]\n",
      " [    7 11600]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "threshold = 1-0.758840\n",
    "predictions = np.where(probs > threshold, 1, 0)\n",
    "\n",
    "# Evaluate the model using various metrics\n",
    "precision = precision_score(y, predictions)\n",
    "recall = recall_score(y, predictions)\n",
    "f1 = f1_score(y, predictions)\n",
    "accuracy = accuracy_score(y, predictions)\n",
    "roc_auc = roc_auc_score(y, probs)\n",
    "\n",
    "# Calculate false negative rate and false positive rate\n",
    "tn, fp, fn, tp = confusion_matrix(y, predictions).ravel()\n",
    "false_negative_rate = fn / (fn + tp)\n",
    "false_positive_rate = fp / (fp + tn)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"ROC AUC Score:\", roc_auc)\n",
    "print(\"False Negative Rate:\", false_negative_rate)\n",
    "print(\"False Positive Rate:\", false_positive_rate)\n",
    "\n",
    "# Print the confusion matrix\n",
    "confusion_mat = confusion_matrix(y, predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_model.joblib']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump((model, threshold),'svm_model.joblib' )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model, thresh = joblib.load('svm_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
